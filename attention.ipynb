{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jV_3puX-v9E"
      },
      "outputs": [],
      "source": [
        "import  os\n",
        "import  numpy               as np\n",
        "import  tensorflow          as tf\n",
        "from    tensorflow          import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "qF-J5lufn6Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "rJBboMbouefN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://nlp.stanford.edu/projects/glove"
      ],
      "metadata": {
        "id": "fe8KXyV53_9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file  = \"glove.6B.50d.txt\"\n",
        "\n",
        "!wget -O {glove_file} https://www.dropbox.com/scl/fi/y328s9lbz8c9glp7al02p/glove.6B.50d.txt?rlkey=m81pwe06f8tpb947fl3y78nlo&dl=0"
      ],
      "metadata": {
        "id": "8_FPiaCbn51G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_glove( glove_file ):\n",
        "    \"\"\"\n",
        "    Read the embedding file.\n",
        "    Return a dict with words as keys and embedding vectors as value.\n",
        "\n",
        "    params:\n",
        "        glove_file  [str]\n",
        "    return:\n",
        "        [dict]      key are [str] and values are [np.array]\n",
        "    \"\"\"\n",
        "    embedding   = {}\n",
        "\n",
        "    with open( glove_file, 'r' ) as f:\n",
        "        cnt = 0\n",
        "\n",
        "        for l in f:\n",
        "            word, vector        = l.split( maxsplit=1 )\n",
        "            vector              = np.fromstring( vector ,sep=' ' )\n",
        "            embedding[ word ]   = vector\n",
        "\n",
        "            cnt += 1\n",
        "            if not cnt % 10000:\n",
        "                print( f\"read {cnt:,} of 400,000 words\" )\n",
        "\n",
        "    print( \"Done!\" )\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "Y2D7BPkvpPOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes about 2m 30s with the short file (the long file takes about 15m)\n",
        "embedding   = read_glove( glove_file )"
      ],
      "metadata": {
        "id": "EMgMol9CpRW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "aKSlOUeSh4-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed( word ):\n",
        "    \"\"\"\n",
        "    Check if a word exists and return its embedding vector\n",
        "    \"\"\"\n",
        "    if isinstance( word, str ):\n",
        "        if word not in embedding.keys():\n",
        "            return False\n",
        "        return embedding[ word ]\n",
        "    return word"
      ],
      "metadata": {
        "id": "UCCH1X_2n-df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim( word1, word2 ):\n",
        "    \"\"\"\n",
        "    Compute similarity between two words.\n",
        "    Return real number in range [-1, 1].\n",
        "    (-1 = max similarity, 1 = no similarity).\n",
        "    \"\"\"\n",
        "    word1   = embed( word1 )\n",
        "    word2   = embed( word2 )\n",
        "    s       = keras.losses.cosine_similarity( word1, word2 )\n",
        "    return s.numpy()"
      ],
      "metadata": {
        "id": "4rhGf6Myok8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plus( word1, word2 ):\n",
        "    \"\"\"\n",
        "    Compute the sum of the embedding vectors of two words\n",
        "    \"\"\"\n",
        "    word1   = embed( word1 )\n",
        "    word2   = embed( word2 )\n",
        "    return word1 + word2"
      ],
      "metadata": {
        "id": "Xea7UFBJpGaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minus( word1, word2 ):\n",
        "    \"\"\"\n",
        "    Compute the difference of the embedding vectors of two words\n",
        "    \"\"\"\n",
        "    word1   = embed( word1 )\n",
        "    word2   = embed( word2 )\n",
        "    return word1 - word2"
      ],
      "metadata": {
        "id": "PfNhwqUppPXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closest( word, n_words=5, limit=50000 ):\n",
        "    \"\"\"\n",
        "    Given a word, find the N words that are \"closest\" to it.\n",
        "\n",
        "    params:\n",
        "        word                [str or np.array] input word\n",
        "        n_words             [int] closest words to find\n",
        "        limit               [int] max number of words to scan\n",
        "\n",
        "    return:\n",
        "        [list of tuples]    ( word [str], similarity score [float] )\n",
        "    \"\"\"\n",
        "    word      = embed( word )\n",
        "    cnt       = 0\n",
        "    best      = [ ( None, 1.0 ) ]    # list of ( word, score )\n",
        "    for w in embedding.keys():\n",
        "        score       = sim( embedding[ w ], word )\n",
        "        if ( score + 1 ) < 0.05:\n",
        "            continue\n",
        "\n",
        "        for i, ( v, s ) in enumerate( best ):\n",
        "            if score < s:\n",
        "                best.insert( i, ( w, score ) )\n",
        "                del best[ n_words: ]\n",
        "                break\n",
        "\n",
        "        cnt += 1\n",
        "        if not cnt % 1000:\n",
        "            print( f\"checked {cnt:,} of {limit:,} words\" )\n",
        "        if cnt > limit:\n",
        "            print()\n",
        "            break\n",
        "\n",
        "    return best"
      ],
      "metadata": {
        "id": "EpFX__8cpegA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "OFz6U3jMuYKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed( 'unicorn' )"
      ],
      "metadata": {
        "id": "Es-YgfxEpiGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim( 'dog', 'wolf' )"
      ],
      "metadata": {
        "id": "uD3IY480pij-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim( 'dog', 'galaxy' )"
      ],
      "metadata": {
        "id": "pW2sVddqplR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest( 'queen', n_words=10, limit=20000 )"
      ],
      "metadata": {
        "id": "FvAeqBUXpnXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = plus( 'woman', minus( 'king', 'man' ) )\n",
        "sim( w, 'queen' )"
      ],
      "metadata": {
        "id": "VYdIGjCgraFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = plus( 'go', minus( 'did', 'do' ) )\n",
        "closest( w, n_words=10, limit=20000 )"
      ],
      "metadata": {
        "id": "P5PoB5Jsr_XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = plus( 'gun', minus( 'person', 'goodness' ) )\n",
        "closest( w, n_words=10, limit=20000 )"
      ],
      "metadata": {
        "id": "8z6p4XBWrgwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "Y0P65fKtxwr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classes"
      ],
      "metadata": {
        "id": "956LK385maN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-head scaled dot-product attention\n",
        "\n",
        "$A(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$"
      ],
      "metadata": {
        "id": "gx9ejDTljNH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention( keras.layers.Layer ):\n",
        "    \"\"\"\n",
        "    Single-head attention layer using the scaled-dot product mechanism with normalization.\n",
        "    Given the three matrices Query, Key, Value, compute the scores and the attention tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__( self, dim, **kwargs ):\n",
        "        \"\"\"\n",
        "        Initialization of the class\n",
        "\n",
        "        params:\n",
        "            dim         [int] dimension (both internal and embedding)\n",
        "        \"\"\"\n",
        "        super().__init__( **kwargs )\n",
        "        self.dim        = dim\n",
        "\n",
        "\n",
        "    def call( self, q, k, v ):\n",
        "        \"\"\"\n",
        "        Compute the scaled dot-product attention.\n",
        "        Default method for class keras.layers.Layer specifying what the layer does when applied to the input.\n",
        "\n",
        "        params:\n",
        "            q           [tensor]\n",
        "            k           [tensor]\n",
        "            v           [tensor]\n",
        "\n",
        "        return:\n",
        "            [tensor] attention\n",
        "            [tensor] scores after softmax\n",
        "        \"\"\"\n",
        "        scores          = tf.matmul( q, k, transpose_b=True )\n",
        "\n",
        "        # normalization to give variance=1 to scores and att\n",
        "        norm            = tf.math.sqrt( tf.cast( self.dim, tf.float32 ) )\n",
        "        scores         /= norm\n",
        "\n",
        "        scores          = tf.nn.softmax( scores )   # also called \"attention matrix\"\n",
        "        att             = tf.matmul( scores, v )\n",
        "        return att, scores"
      ],
      "metadata": {
        "id": "r9rhb6c1AGfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention( keras.layers.Layer ):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer.\n",
        "    Implement a simple attention mechanism several times in parallel.\n",
        "    The independent attention outputs are concatenated and linearly transformed into the expected dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__( self, dim, n_heads, **kwargs ):\n",
        "        \"\"\"\n",
        "        Initialization of the class.\n",
        "\n",
        "        params:\n",
        "            dim         [int] dimension (both internal and embedding)\n",
        "            n_heads     [int] number of heads\n",
        "        \"\"\"\n",
        "        super().__init__( **kwargs )\n",
        "        self.dim        = dim\n",
        "        self.n_heads    = n_heads\n",
        "        self.attention  = Attention( self.dim )             # internal attention layer\n",
        "\n",
        "        # a Dense layer without activation function is treated as a simple matrix multiplication\n",
        "        self.Q          = keras.layers.Dense( self.dim, activation=None )\n",
        "        self.K          = keras.layers.Dense( self.dim, activation=None )\n",
        "        self.V          = keras.layers.Dense( self.dim, activation=None )\n",
        "        self.O          = keras.layers.Dense( self.dim, activation=None )\n",
        "\n",
        "\n",
        "    def _split_heads( self, x ):\n",
        "        \"\"\"\n",
        "        Split a tensor into heads.\n",
        "        Add a dimension and rearrange the shape.\n",
        "\n",
        "        NOTE!\n",
        "        Using x.shape does not work! The actual shape is given only by tf.shape( x )\n",
        "        \"\"\"\n",
        "\n",
        "        # original shape is [ batch, seq_len, dim ]\n",
        "        # note that 'seq_len' is not passed to the class! it is extracted automatically from the tensors\n",
        "        original_shape  = tf.shape( x )\n",
        "\n",
        "        # new shape is [ batch, seq_len, n_heads, new_dim ]\n",
        "        # note that you don't need to specify the new size thanks to argument '-1' of tf.reshape()\n",
        "        headed_shape    = ( original_shape[ 0 ], original_shape[ 1 ], self.n_heads, -1 )\n",
        "        x               = tf.reshape( x, shape=headed_shape )\n",
        "\n",
        "        # final shape is [ batch, n_heads, seq_len, new_dim ]\n",
        "        x               = tf.transpose( x, perm=( 0, 2, 1, 3 ) )\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _join_heads( self, x ):\n",
        "        \"\"\"\n",
        "        Rejoin a tensor collapsing its heads.\n",
        "        Basically reverts the effect of _split_heads().\n",
        "        \"\"\"\n",
        "        x               = tf.transpose( x, perm=( 0, 2, 1, 3 ) )\n",
        "        headed_shape    = tf.shape( x )\n",
        "        original_shape  = ( headed_shape[ 0 ], headed_shape[ 1 ], self.dim )\n",
        "        x               = tf.reshape( x, shape=original_shape )\n",
        "        return x\n",
        "\n",
        "\n",
        "    def call( self, q, k, v, return_matrix=False ):\n",
        "        \"\"\"\n",
        "        Compute the multi-head attention.\n",
        "        Default method for class keras.layers.Layer specifying what the layer does when applied to the input.\n",
        "\n",
        "        params:\n",
        "            q               [tensor]\n",
        "            k               [tensor]\n",
        "            v               [tensor]\n",
        "            return_matrix   [bool] whether to return the attention matrix\n",
        "\n",
        "        return:\n",
        "            [tensor] attention output\n",
        "            [tensor] OPTIONAL attention matrix (scores)\n",
        "        \"\"\"\n",
        "        q               = self.Q( q )\n",
        "        k               = self.K( k )\n",
        "        v               = self.V( v )\n",
        "\n",
        "        # split attention heads\n",
        "        q               = self._split_heads( q )\n",
        "        k               = self._split_heads( k )\n",
        "        v               = self._split_heads( v )\n",
        "\n",
        "        # compute attentions in parallel\n",
        "        a, matrix       = self.attention( q, k, v )\n",
        "\n",
        "        # join heads and compute result\n",
        "        a               = self._join_heads( a )\n",
        "        o               = self.O( a )\n",
        "\n",
        "        if return_matrix:\n",
        "            return o, matrix\n",
        "        return o"
      ],
      "metadata": {
        "id": "Qt-5VxfDCWc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ],
      "metadata": {
        "id": "jBox2pO4mlzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention with 2 heads and internal dimension 4\n",
        "mha     = MultiHeadAttention( 4, 2 )"
      ],
      "metadata": {
        "id": "h6ascJvyuCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random input sequence of 5 vetors of dimension 4\n",
        "# [ batch=1, seq_len=5, dim=4 ]\n",
        "x       = np.random.random( ( 1, 5, 4 ) )\n",
        "\n",
        "print( x )"
      ],
      "metadata": {
        "id": "FoZSZLxfyOTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the input is passed three times, to compute q, k and v separately\n",
        "out, mtx    = mha( x, x, x, return_matrix=True )"
      ],
      "metadata": {
        "id": "xKjznVXkzq7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the scores organized into the \"attention matrix\"\n",
        "# representing the correlations among the 5 elements of the input sequence\n",
        "\n",
        "# [ batch, n_head, seq_len, seq_len ]\n",
        "print( mtx )"
      ],
      "metadata": {
        "id": "3Upz97IFuhT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the output of the attention layer has the same dimensions of the input\n",
        "# this is an intermediate output and it will be processed further into the Transformer\n",
        "print( out )"
      ],
      "metadata": {
        "id": "and4w5ke1Q_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}