{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XP5EBEuFzQ76",
        "kukFH2m3rtNd",
        "E0JEK6lhyMuw",
        "XHw3i3AYF55s",
        "QaIHcgvOH8OV",
        "KsHC5vCW5k-U",
        "3-gbQhYXLZCy",
        "vdIb3xlWxQzf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text==2.15 keras-nlp==0.8.2"
      ],
      "metadata": {
        "id": "uuCGfv9xvHbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jV_3puX-v9E"
      },
      "outputs": [],
      "source": [
        "import  os\n",
        "import  numpy               as np\n",
        "import  tensorflow          as tf\n",
        "from    tensorflow          import keras\n",
        "import  keras_nlp\n",
        "import  matplotlib.pyplot   as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "rpufQ46Mx7tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n"
      ],
      "metadata": {
        "id": "XP5EBEuFzQ76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleBooks dataset for training and the Tokenizer\n",
        "\n",
        "https://arxiv.org/pdf/1911.12391.pdf\n",
        "\n",
        "https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip"
      ],
      "metadata": {
        "id": "-_zxKdQI7DyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset_train_f    = \"simplebooks-92-raw_train.txt\"\n",
        "dset_valid_f    = \"simplebooks-92-raw_valid.txt\"\n",
        "dset_test_f     = \"simplebooks-92-raw_test.txt\"\n",
        "\n",
        "!wget -O {dset_train_f} https://www.dropbox.com/scl/fi/r6vnn7vccpvscvmzabmvf/simplebooks-92-raw_train.txt?rlkey=thwnurvjrda737sr8283qpdkg&dl=0\n",
        "!wget -O {dset_valid_f} https://www.dropbox.com/scl/fi/txlel36qe8nxz7jth95xp/simplebooks-92-raw_valid.txt?rlkey=8sdbtycdx2ppokcybaolvrx7j&dl=0\n",
        "!wget -O {dset_test_f} https://www.dropbox.com/scl/fi/3j9eifu45sdwnhof3r63q/simplebooks-92-raw_test.txt?rlkey=5c8z385wsu925h9zugz7p2d8v&dl=0"
      ],
      "metadata": {
        "id": "E9vZCtmTx_Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ready-to-use models and vocabulary\n",
        "!wget -O vocab_10000.txt https://www.dropbox.com/scl/fi/aix95cadh32i9ylzjzz0m/vocab_10000.txt?rlkey=qcwpxhw34c8z2x7hue37zu79d&dl=0\n",
        "!wget -O transf_v10000_s128_h4_e50.h5 https://www.dropbox.com/scl/fi/la654e3fsjfn6iwrlmm1d/transf_v10000_s128_h4_e50.h5?rlkey=9qkdroty1i2l9lu1euaxjtkvg&dl=0\n",
        "!wget -O transf_v10000_s128_h4_e200.h5 https://www.dropbox.com/scl/fi/tvevf1ocgatw2v0nl624f/transf_v10000_s128_h4_e200.h5?rlkey=0pxwmhyicvkyxdae3jsc0znsb&dl=0"
      ],
      "metadata": {
        "id": "VMdeipjL5sVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "kukFH2m3rtNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_TRAIN_SEQ_LEN       = 400       # select only long samples (about 200k in the end)\n",
        "VOCAB_SIZE              = 10000     # size of the vocabulary\n",
        "SEQ_LEN                 = 128       # maximum sequence length accepted\n",
        "\n",
        "EMBED_DIM               = 128       # dimension for word embedding\n",
        "NUM_HEADS               = 4         # number of attention heads\n",
        "\n",
        "EPOCHS                  = 200       # number of training epochs\n",
        "BATCH_SIZE              = 64        # batch size for training"
      ],
      "metadata": {
        "id": "zQheM1U5zWmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and dataset"
      ],
      "metadata": {
        "id": "E0JEK6lhyMuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dset():\n",
        "    \"\"\"\n",
        "    Use the SimpleBooks dataset for training and validation.\n",
        "    To be invoked olny when training a new model.\n",
        "\n",
        "    Consider only sentences with a minimum length.\n",
        "\n",
        "    return:\n",
        "        [tuple] of two [tf.data.TextLineDataset], one for trainig and one for validation\n",
        "    \"\"\"\n",
        "\n",
        "    dset_train  = tf.data.TextLineDataset( dset_train_f )\n",
        "    dset_train  = dset_train.filter( lambda x: tf.strings.length( x ) > MIN_TRAIN_SEQ_LEN )\n",
        "    dset_train  = dset_train.batch( BATCH_SIZE )\n",
        "    dset_train  = dset_train.shuffle( buffer_size=1024 )\n",
        "\n",
        "    dset_valid  = tf.data.TextLineDataset( dset_valid_f )\n",
        "    dset_valid  = dset_valid.filter( lambda x: tf.strings.length( x ) > MIN_TRAIN_SEQ_LEN )\n",
        "    dset_valid  = dset_valid.batch( BATCH_SIZE )\n",
        "\n",
        "    return dset_train, dset_valid"
      ],
      "metadata": {
        "id": "8te1_UqoyPLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_vocab( dset ):\n",
        "    \"\"\"\n",
        "    Given a dataset, create the vocabulary file.\n",
        "\n",
        "    input:\n",
        "        dset:       [tf.data.TextLineDataset] dataset on which the vocabulary is computed\n",
        "\n",
        "    return:\n",
        "        [list] the vocabulary, a list of words\n",
        "    \"\"\"\n",
        "\n",
        "    fname   = f\"vocab_{VOCAB_SIZE}.txt\"\n",
        "\n",
        "    if os.path.isfile( fname ):\n",
        "        vocab   = load_vocab( fname=fname )\n",
        "        return vocab\n",
        "\n",
        "    vocab   = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        dset,\n",
        "        vocabulary_size = VOCAB_SIZE,\n",
        "        lowercase       = True,\n",
        "        reserved_tokens = [ \"[PAD]\", \"[UNK]\", \"[BOS]\" ]\n",
        "    )\n",
        "\n",
        "    with open( fname, 'w' ) as f:\n",
        "        f.write( \"\\n\".join( w for w in vocab ) )\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "dXMZ1-GkzyTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab( fname=None ):\n",
        "    \"\"\"\n",
        "    Load the vocabulary from a file.\n",
        "\n",
        "    input:\n",
        "        fname:      [str] file name, if None is used a convention based on VOCAB_SIZE\n",
        "\n",
        "    return:\n",
        "        [list] the vocabulary, a list of words\n",
        "    \"\"\"\n",
        "\n",
        "    if fname is None:\n",
        "        fname   = f\"vocab_{VOCAB_SIZE}.txt\"\n",
        "\n",
        "    with open( fname, 'r' ) as f:\n",
        "        vocab   = f.read()\n",
        "    vocab   = vocab.split()\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "lwwwBZFA12w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_tokenizer( vocab ):\n",
        "    \"\"\"\n",
        "    Given a vocabulary, set a Tokenizer.\n",
        "\n",
        "    input:\n",
        "        vocab:      [list] vocabulary\n",
        "\n",
        "    return:\n",
        "        [keras_nlp.tokenizers.Tokenizer]\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer   = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "        vocabulary      = vocab,\n",
        "        sequence_length = SEQ_LEN,  # maximum length of acceptable input sequence\n",
        "        lowercase       = True\n",
        "    )\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "QblLHumK6sGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dset( tokenizer, dset_tr, dset_vl ):\n",
        "    \"\"\"\n",
        "    Apply the tokenization to the training and validation sets\n",
        "\n",
        "    input:\n",
        "        tokenizer:  [keras_nlp.tokenizers.Tokenizer]\n",
        "        tset:       [tf.data.TextLineDataset] training set\n",
        "        vset:       [tf.data.TextLineDataset] validation set\n",
        "\n",
        "    return:\n",
        "        [tuple] of [tf.data.TextLineDataset] tokenized sets\n",
        "    \"\"\"\n",
        "\n",
        "    # adds a start token [BOS] and make all sequences the same length by adding padding tokens [PAD]\n",
        "    start_packer    = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length = SEQ_LEN,\n",
        "        start_value     = tokenizer.token_to_id( '[BOS]' )\n",
        "    )\n",
        "\n",
        "    def sfunc( inputs ):\n",
        "        \"\"\"\n",
        "        Support function to apply tokenization and packing to a sample\n",
        "        \"\"\"\n",
        "        outputs     = tokenizer( inputs )\n",
        "        features    = start_packer( outputs )\n",
        "        return features, outputs\n",
        "\n",
        "    # tokenize train and validation sequences in parallel\n",
        "    dset_train  = dset_tr.map( sfunc, num_parallel_calls=tf.data.AUTOTUNE )\n",
        "    dset_train  = dset_train.prefetch( tf.data.AUTOTUNE )\n",
        "    dset_valid  = dset_vl.map( sfunc, num_parallel_calls=tf.data.AUTOTUNE )\n",
        "    dset_valid  = dset_valid.prefetch( tf.data.AUTOTUNE )\n",
        "\n",
        "    return dset_train, dset_valid"
      ],
      "metadata": {
        "id": "rBe_vCv_7LI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer usage"
      ],
      "metadata": {
        "id": "_zCZMDxVBjqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset_train, dset_valid  = load_dset()                           # load dataset\n",
        "vocab                   = set_vocab( dset_train )               # create vocabulary\n",
        "# vocab                   = load_vocab( \"vocab_10000.txt\" )     # ...or read it from file\n",
        "\n",
        "tokenizer               = set_tokenizer( vocab )\n",
        "tset_train, tset_valid  = tokenize_dset( tokenizer, dset_train, dset_valid )"
      ],
      "metadata": {
        "id": "3DQFjqoD3HvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s   = \"the cat in on the table\"\n",
        "t   = tokenizer.tokenize( s )\n",
        "print( t )"
      ],
      "metadata": {
        "id": "_nrp5vEOz8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t   = tokenizer.detokenize( [ 1321 ] )\n",
        "print( t )"
      ],
      "metadata": {
        "id": "NZ27mZG60EUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w   = 'yellowish'\n",
        "t   = tokenizer.tokenize( w )\n",
        "print( t )"
      ],
      "metadata": {
        "id": "d6Kgh2T2zYTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1  = tokenizer.detokenize( [ 1064 ] )\n",
        "t2  = tokenizer.detokenize( [ 975 ] )\n",
        "print( t1 )\n",
        "print( t2 )"
      ],
      "metadata": {
        "id": "jbQU0DLfzdx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w   = 'unicorn'\n",
        "t   = tokenizer.tokenize( w )\n",
        "print( t )"
      ],
      "metadata": {
        "id": "Ag3xO4e2xnyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1  = tokenizer.detokenize( [ 5028 ] )\n",
        "t2  = tokenizer.detokenize( [ 5039 ] )\n",
        "t3  = tokenizer.detokenize( [ 3608 ] )\n",
        "print( t1 )\n",
        "print( t2 )\n",
        "print( t3 )"
      ],
      "metadata": {
        "id": "AuT0cMNzyzjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Class"
      ],
      "metadata": {
        "id": "XHw3i3AYF55s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransDecoder( object ):\n",
        "    \"\"\"\n",
        "    Simple Transformer decoder using the attention layer from Keras\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__( self, num_heads=NUM_HEADS, length=SEQ_LEN, dim=EMBED_DIM, summary=False ):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            num_heads   [int] number of attention heads\n",
        "            length      [int] length of the input sequence\n",
        "            dim         [int] embedding dimension\n",
        "            summary     [bool] whether to generate a summary of the created model\n",
        "        \"\"\"\n",
        "        self.num_heads  = num_heads\n",
        "        self.length     = length\n",
        "        self.dim        = dim\n",
        "        self.key_dim    = dim // num_heads          # new internal dimension after the head splitting\n",
        "        self.embedding  = self._embedding()         # embedding layer\n",
        "        self.attention  = self._attention_keras()   # attention layer\n",
        "        self.model      = self.create_model()       # complete model\n",
        "\n",
        "\n",
        "\n",
        "    def _embedding( self ):\n",
        "        \"\"\"\n",
        "        Define the embedding layer of the model.\n",
        "        Embeds both positions and tokens.\n",
        "        \"\"\"\n",
        "        embedding   = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size = VOCAB_SIZE,\n",
        "            sequence_length = self.length,\n",
        "            embedding_dim   = self.dim,\n",
        "            mask_zero       = True          # use the value 0 as padding token [PAD]\n",
        "        )\n",
        "        return embedding\n",
        "\n",
        "\n",
        "\n",
        "    def _attention_keras( self ):\n",
        "        \"\"\"\n",
        "        Define the attention layer of the model.\n",
        "        Use the default multi-head attention layer in Keras.\n",
        "        \"\"\"\n",
        "        att         = keras.layers.MultiHeadAttention(\n",
        "                num_heads   = self.num_heads,\n",
        "                key_dim     = self.key_dim\n",
        "        )\n",
        "        return att\n",
        "\n",
        "\n",
        "\n",
        "    def create_model( self ):\n",
        "        \"\"\"\n",
        "        Create a Transformer model with a single attention layer.\n",
        "        \"\"\"\n",
        "        inputs      = keras.layers.Input( shape=( None, ), dtype=tf.int32 )     # input are token indices\n",
        "        x           = self.embedding( inputs )                                  # token and position embedding\n",
        "        x           = self.attention( x, x, x )                                 # attention layer\n",
        "        x           = keras.layers.LayerNormalization()( x )                    # normalization layer\n",
        "        outputs     = keras.layers.Dense( VOCAB_SIZE )( x )                     # final logits layer\n",
        "        model       = keras.Model( inputs=inputs, outputs=outputs )             # complete model\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    def attention_matrix( self, x ):\n",
        "        \"\"\"\n",
        "        Extract the attention matrix, containing the scores.\n",
        "\n",
        "        params:\n",
        "            x       [tf.Tensor] tokenized input sequence\n",
        "\n",
        "        return:\n",
        "            [tf.Tensor] attention scores\n",
        "        \"\"\"\n",
        "        if x.ndim < 2:  # add batch dimension if necessary\n",
        "            x       = x[ tf.newaxis, : ]\n",
        "\n",
        "        x           = self.embedding( x )\n",
        "\n",
        "        # set 'return_attention_scores' to True to have both results (output and scores)\n",
        "        # set 'use_causal_mask' to True to use the Trasformer as decoder, masking future tokens in the sequence\n",
        "        out, scores = self.attention( x, x, x, return_attention_scores=True, use_causal_mask=True )\n",
        "        return scores"
      ],
      "metadata": {
        "id": "0_62JSv7F9U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for training"
      ],
      "metadata": {
        "id": "QaIHcgvOH8OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model( model, tr_set, vl_set ):\n",
        "    \"\"\"\n",
        "    Train the model\n",
        "\n",
        "    params:\n",
        "        model   [keras.Model]\n",
        "        tset    [tf.data.TextLineDataset] training set\n",
        "        vset    [tf.data.TextLineDataset] validation set\n",
        "\n",
        "    return:\n",
        "        [tf.History] training log\n",
        "    \"\"\"\n",
        "\n",
        "    # categorical cross entropy loss to minimize during training\n",
        "    # the model output is considered as a logits tensor\n",
        "    loss        = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True )\n",
        "\n",
        "    # perplexity metric to evaluate the performance of the model\n",
        "    # the model output is considered as a logits tensor, and the token '0' is masked for padding\n",
        "    perplexity  = keras_nlp.metrics.Perplexity( from_logits=True, mask_token_id=0 )\n",
        "\n",
        "    model.compile( optimizer='adam', loss=loss, metrics=[ perplexity ] )\n",
        "    history     = model.fit( tr_set, validation_data=vl_set, verbose=1, epochs=EPOCHS )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "dW6P9sh1IBXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model( model, history, name=None ):\n",
        "    \"\"\"\n",
        "    Save a trained model\n",
        "\n",
        "    params:\n",
        "        model   [keras.Model]\n",
        "        history [tf.History] training log\n",
        "        name    [str] filename where to save the model, if None generate it automatically\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = f\"transf_v{VOCAB_SIZE}_s{SEQ_LEN}_h{NUM_HEADS}_e{EPOCHS}\"\n",
        "\n",
        "    # make sure that every layer in the model has an unique name\n",
        "    try:\n",
        "        model.save_weights( name + \".h5\" )\n",
        "    except ValueError:\n",
        "        for i in range( len( model.weights ) ):\n",
        "            model.weights[ i ]._handle_name = f\"{model.weights[ i ].name}_{i}\"\n",
        "        model.save_weights( name + \".h5\" )\n",
        "\n",
        "    his         = history.history\n",
        "    fmt         = \"{:04d}   {:^9.5f} {:^9.5f} {:^9.3f} {:^9.3f}\\n\"\n",
        "\n",
        "    # save a formatted training log\n",
        "    with open( name + \".log\", 'w' ) as f:\n",
        "        f.write( \"epoch    loss    prplx  val_loss val_prplx\\n\" )\n",
        "        f.write( 60 * '=' + '\\n' )\n",
        "        for e in history.epoch:\n",
        "            f.write( fmt.format( e, his[ 'loss' ][e], his[ 'prplx' ][e], his[ 'val_loss' ][e], his[ 'val_prplx' ][e] ) )\n",
        "        f.write( 60 * '=' + '\\n' )"
      ],
      "metadata": {
        "id": "IAZxQHScLAbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for testing"
      ],
      "metadata": {
        "id": "KsHC5vCW5k-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention( model, tokenizer, prompt ):\n",
        "    \"\"\"\n",
        "    Extract the attention matrices containing the scores\n",
        "\n",
        "    params:\n",
        "        model       [keras_nlp.model]\n",
        "        tokenizer   [keras_nlp.tokenizers]\n",
        "        prompt      [str] input prompt\n",
        "\n",
        "    return:\n",
        "        [tf.Tensor] attention matrices\n",
        "    \"\"\"\n",
        "    # convert the text prompt into list of words\n",
        "    prompt_list     = [ '[BOS]' ] + prompt.lower().split()\n",
        "\n",
        "    # convert the list of words into list of tokens\n",
        "    prompt_tokens   = [ tokenizer.token_to_id( w ) for w in prompt_list ]\n",
        "\n",
        "    # convert the list of tokens into a tensor, with batch dimension\n",
        "    prompt_tokens   = tf.convert_to_tensor( prompt_tokens )\n",
        "    prompt_tokens   = prompt_tokens[ tf.newaxis, : ]\n",
        "\n",
        "    att_matrix      = model.attention_matrix( prompt_tokens )\n",
        "    return att_matrix[ 0 ]  # remove batch dimension"
      ],
      "metadata": {
        "id": "NCDEedJhJA6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention( attention, prompt ):\n",
        "    \"\"\"\n",
        "    Plot the attention matrices\n",
        "\n",
        "    params:\n",
        "        attention   [tf.Tensor] attention matrices returned by get_attention()\n",
        "        prompt      [str] input prompt\n",
        "    \"\"\"\n",
        "    # convert the text prompt into list of words\n",
        "    prompt_list     = [ '[BOS]' ] + prompt.lower().split()\n",
        "\n",
        "    # a subplot for each attention\n",
        "    fig_size        = ( NUM_HEADS * 5, 5 )\n",
        "    fig, axes       = plt.subplots( 1, NUM_HEADS, figsize=fig_size )\n",
        "\n",
        "    # plot each attention head in a subplot\n",
        "    for h, head in enumerate( attention ):\n",
        "        axis = axes[ h ] if NUM_HEADS > 1 else axes     # select the appropriate axis for the subplot\n",
        "        axis.matshow( head.numpy() )\n",
        "        axis.set_xticks( range( len( prompt_list ) ) )\n",
        "        axis.set_yticks( range( len( prompt_list ) ) )\n",
        "        axis.set_xticklabels( prompt_list, rotation=90 )\n",
        "        axis.set_yticklabels( prompt_list )\n",
        "\n",
        "    plt.tight_layout()  # adjust layout to prevent overlapping\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5ZWI3FvdrlM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "3-gbQhYXLZCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train a new model from scratch or load weights from a pre-trained model\n",
        "TRAIN   = False"
      ],
      "metadata": {
        "id": "HZz8iQ_AubRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transf  = TransDecoder()\n",
        "\n",
        "if TRAIN:\n",
        "    history     = train_model( transf.model, tset_train, tset_valid )\n",
        "    save_model( transf.model, history )\n",
        "\n",
        "else:\n",
        "    model_name  = \"transf_v10000_s128_h4_e200.h5\"\n",
        "    transf.model.load_weights( model_name )"
      ],
      "metadata": {
        "id": "5TZCG0k9LbST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt      = \"the cat on the table is black\"\n",
        "\n",
        "att         = get_attention( transf, tokenizer, prompt )\n",
        "plot_attention( att, prompt )"
      ],
      "metadata": {
        "id": "7o2nK4R6N1dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple GPT\n",
        "\n",
        "It uses the same dataset, vocabulary, tokenizer, and training functions of the *Transformer* example above"
      ],
      "metadata": {
        "id": "Hos33lCCv0tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "vdIb3xlWxQzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O gpt_v10000_s128_l2_h4_e50.h5 https://www.dropbox.com/scl/fi/ryn31mv5ne6k9mc18kffh/gpt_v10000_s128_l2_h4_e50.h5?rlkey=p8x4bhe5oxb8lpw3f54i970ud&dl=0\n",
        "!wget -O gpt_v10000_s128_l2_h4_e100.h5 https://www.dropbox.com/scl/fi/ie0y2rk65z2encjhj203x/gpt_v10000_s128_l2_h4_e100.h5?rlkey=xo46jcgl1qywu5fm9bpzclg0z&dl=0"
      ],
      "metadata": {
        "id": "09e6e14-xR2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {
        "id": "H2BsNeLqwMtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_TRAIN_SEQ_LEN       = 400       # select only long samples (about 200k in the end)\n",
        "VOCAB_SIZE              = 10000     # size of the vocabulary\n",
        "SEQ_LEN                 = 128       # maximum sequence length accepted\n",
        "OUT_TOKENS              = 80        # default number of tokens to generate for completion\n",
        "\n",
        "EMBED_DIM               = 256       # dimension for word embedding\n",
        "NUM_HEADS               = 4         # number of attention heads\n",
        "NUM_LAYERS              = 2         # number of Transformer decoder blocks\n",
        "MLP_DIM                 = 256       # size of the dense layer in the Transformer decoder\n",
        "\n",
        "EPOCHS                  = 100       # number of training epochs\n",
        "BATCH_SIZE              = 64        # batch size for training"
      ],
      "metadata": {
        "id": "QP_pGc9-wN3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "Q7LzQR7qw7Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Create a simple GPT model using several Transformer blocks from Keras\n",
        "\n",
        "    return:\n",
        "        [keras.Model]\n",
        "    \"\"\"\n",
        "\n",
        "    # input layer representing the sequence of tokens (sequence length doesn't need to be specified)\n",
        "    inputs      = keras.layers.Input( shape=( None, ), dtype=tf.int32 )\n",
        "\n",
        "    # token and position embedding layer\n",
        "    embedding   = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size = VOCAB_SIZE,\n",
        "        sequence_length = SEQ_LEN,\n",
        "        embedding_dim   = EMBED_DIM,\n",
        "        mask_zero       = True          # use the value 0 as padding token [PAD]\n",
        "    )\n",
        "    x           = embedding( inputs )\n",
        "\n",
        "    # Transformer decoder blocks\n",
        "    for i in range( NUM_LAYERS ):\n",
        "        name    = \"decoder_{:02d}\".format( i )\n",
        "        decoder = keras_nlp.layers.TransformerDecoder(\n",
        "            num_heads           = NUM_HEADS,\n",
        "            intermediate_dim    = MLP_DIM\n",
        "        )\n",
        "        x       = decoder( x )\n",
        "\n",
        "    # final dense layer with the logits tensor\n",
        "    outputs     = keras.layers.Dense( VOCAB_SIZE )( x )\n",
        "\n",
        "    # complete model\n",
        "    model       = keras.Model( inputs=inputs, outputs=outputs )\n",
        "    return model"
      ],
      "metadata": {
        "id": "YPhLydZRw0VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for prediction"
      ],
      "metadata": {
        "id": "ng9wv0uE5SCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token( text, tokenizer ):\n",
        "    \"\"\"\n",
        "    Transform a text into tokens\n",
        "\n",
        "    params:\n",
        "        text        [str]\n",
        "        tokenizer   [keras_nlp.tokenizers]\n",
        "\n",
        "    return:\n",
        "        [np.array] array of int\n",
        "    \"\"\"\n",
        "    tokens      = tokenizer( text.lower() )\n",
        "    tokens      = tokens.numpy()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "IWLETu8yyHA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_to_text( tokens, tokenizer ):\n",
        "    \"\"\"\n",
        "    Transform tokens into text\n",
        "\n",
        "    params:\n",
        "        tokens      [tf.Tensor] tensor of int\n",
        "        tokenizer   [keras_nlp.tokenizers]\n",
        "\n",
        "    return:\n",
        "        [bytes] string in bytes format\n",
        "    \"\"\"\n",
        "    text        = tokenizer.detokenize( tokens )\n",
        "    text        = text.numpy()\n",
        "    return text"
      ],
      "metadata": {
        "id": "nNn5M1NZ4RQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict( model, tokenizer, prompt, k=1, view_steps=True ):\n",
        "    \"\"\"\n",
        "    Use the model to generate predictions from a prompt.\n",
        "\n",
        "    Given a prompt of N tokens, the model generate a sequence of N output tokens.\n",
        "    To predict the i-th output token, the model consider the prompt tokens in [ 0, i-1 ].\n",
        "\n",
        "    input:\n",
        "        model       [keras_nlp.model]\n",
        "        tokenizer   [keras_nlp.tokenizers]\n",
        "        prompt      [str]\n",
        "        k           [int] how many options to output\n",
        "\n",
        "    return:\n",
        "        [array] of [bytes] strings in bytes format\n",
        "    \"\"\"\n",
        "    prompt_bos      = [ tokenizer.token_to_id( '[BOS]' ) ]                                  # token for [BOS]\n",
        "    prompt_tokens   = tokenizer( prompt.lower() )                                           # tokenize the prompt\n",
        "    prompt_tokens   = tf.squeeze( tf.gather( prompt_tokens, tf.where( prompt_tokens ) ) )   # remove [PAD] tokens\n",
        "    prompt_tokens   = tf.concat( [ prompt_bos, prompt_tokens ], axis=0 )                    # add [BOS] token\n",
        "    prompt_tokens_b = prompt_tokens[ tf.newaxis, : ]                                        # add batch dimension\n",
        "\n",
        "    # call the model on the tokenized prompt to produce a logits vector\n",
        "    prediction      = model( prompt_tokens_b )\n",
        "\n",
        "    # use top_k to select the best k tokens from the logits prediction\n",
        "    # (returns the indices of the tokens in the logits vector)\n",
        "    out_tokens      = tf.math.top_k( prediction, k=k )[ 1 ][ 0 ]\n",
        "\n",
        "    # convert the tokens into text\n",
        "    # out   = token_to_text( sel_tokens, tokenizer )\n",
        "    # print( sel_tokens[ 0 ].shape )\n",
        "\n",
        "    if view_steps:\n",
        "        for i in range( len( prompt_tokens ) ):\n",
        "            p   = token_to_text( prompt_tokens[ :i+1 ], tokenizer )\n",
        "            p   = p.decode( 'utf-8' )\n",
        "            print( f\"Prompt: { p }\" )\n",
        "            for j, t in enumerate( out_tokens[ i ].numpy() ):\n",
        "                o   = token_to_text( [ t ], tokenizer )\n",
        "                o   = o.decode( 'utf-8' )\n",
        "                print( f\"Output { j+1 }: { o }\" )\n",
        "            print()\n",
        "\n",
        "    return token_to_text( out_tokens, tokenizer )"
      ],
      "metadata": {
        "id": "YRoc4nRZzfUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete( model, tokenizer, prompt, method, max_length=OUT_TOKENS ):\n",
        "    \"\"\"\n",
        "    Use the model to make completions of a prompt.\n",
        "\n",
        "    input:\n",
        "        model       [keras_nlp.model]\n",
        "        tokenizer   [keras_nlp.tokenizers]\n",
        "        prompt      [str]\n",
        "        method      [str] one of Keras Samplers https://keras.io/api/keras_nlp/samplers/\n",
        "                    'greedy', 'beam', 'top_k', 'top_p'\n",
        "    return:\n",
        "        [bytes] str in bytes format\n",
        "    \"\"\"\n",
        "    prompt_bos      = [ tokenizer.token_to_id( '[BOS]' ) ]                                  # token for [BOS]\n",
        "    prompt_tokens   = tokenizer( prompt.lower() )                                           # tokenize the prompt\n",
        "    prompt_tokens   = tf.concat( [ prompt_bos, prompt_tokens[ :-1 ] ], axis=0 )             # add [BOS] token\n",
        "    prompt_tokens_b = prompt_tokens[ tf.newaxis, : ]                                        # add batch dimension\n",
        "    prompt_len      = tf.math.count_nonzero( prompt_tokens, dtype=tf.int32 ).numpy()        # count non [PAD] tokens\n",
        "\n",
        "    # available Sampler algorithms\n",
        "    sampler = None\n",
        "    if method == 'greedy':\n",
        "        sampler     = keras_nlp.samplers.GreedySampler()\n",
        "    elif method == 'beam':\n",
        "        sampler     = keras_nlp.samplers.BeamSampler( num_beams=10 )\n",
        "    elif method == 'top_k':\n",
        "        sampler     = keras_nlp.samplers.TopKSampler( k=10 )\n",
        "    elif method == 'top_p':\n",
        "        sampler     = keras_nlp.samplers.TopPSampler( p=0.5 )\n",
        "\n",
        "    # support function to use with Sampler calls\n",
        "    def predict_fn( inputs, cache, index ):\n",
        "        logits          = model( inputs )[ :, index-1, : ]\n",
        "        hidden_states   = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # call the Sampler to make the completion\n",
        "    output_tokens   = sampler(\n",
        "        next            = predict_fn,\n",
        "        prompt          = prompt_tokens_b,\n",
        "        index           = prompt_len\n",
        "    )\n",
        "\n",
        "    text    = token_to_text( output_tokens, tokenizer )\n",
        "    text    = text[ 0 ].decode( 'utf-8' )\n",
        "    return text"
      ],
      "metadata": {
        "id": "K3d5EDEWzkPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ],
      "metadata": {
        "id": "lKhLBizGxCfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset_train, dset_valid  = load_dset()\n",
        "vocab                   = set_vocab( dset_train )\n",
        "tokenizer               = set_tokenizer( vocab )\n",
        "tset_train, tset_valid  = tokenize_dset( tokenizer, dset_train, dset_valid )"
      ],
      "metadata": {
        "id": "lHD7tsNjw6S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a new model from scratch or load weights from a pre-trained model\n",
        "TRAIN   = False"
      ],
      "metadata": {
        "id": "tMoeRxE47GU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model   = create_model()\n",
        "\n",
        "if TRAIN:\n",
        "    history     = train_model( model, tset_train, tset_valid )\n",
        "    save_model( model, history )\n",
        "\n",
        "else:\n",
        "    model_name  = \"gpt_v10000_s128_l2_h4_e100.h5\"\n",
        "    model.load_weights( model_name )"
      ],
      "metadata": {
        "id": "JvMSv9wn7MBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"on the table, there is a yellow\""
      ],
      "metadata": {
        "id": "C9kb6-Q46Pwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = predict( model, tokenizer, prompt=prompt, k=3 )"
      ],
      "metadata": {
        "id": "AVVgk-Ut6bXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length  = 80"
      ],
      "metadata": {
        "id": "AkMf_2zVMAPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete( model, tokenizer, prompt, method='greedy', max_length=max_length )"
      ],
      "metadata": {
        "id": "BaqYyxCU647i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete( model, tokenizer, prompt, method='beam', max_length=max_length )"
      ],
      "metadata": {
        "id": "OmVDSUpLMays"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete( model, tokenizer, prompt, method='top_k', max_length=max_length )"
      ],
      "metadata": {
        "id": "KtGGoAe4MbYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete( model, tokenizer, prompt, method='top_p', max_length=max_length )"
      ],
      "metadata": {
        "id": "3o3yWsmGMb7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}